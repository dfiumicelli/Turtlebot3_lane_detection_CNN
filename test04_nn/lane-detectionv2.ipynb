{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6137700,"sourceType":"datasetVersion","datasetId":3519327}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile train.py\n# train_METRICS_FIXED.py - FIX CRÍTICO DELLE METRICHE\n\nimport os\nimport warnings\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nimport segmentation_models_pytorch as smp\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', message='.*UnsupportedFieldAttributeWarning.*')\n\nfrom dataset import LaneSegmentationDataset\nfrom augmentation import (\n    get_training_augmentation,\n    get_validation_augmentation,\n)\nfrom metrics import LaneMetrics\n\n# ==================== CONFIGURAZIONE KAGGLE ====================\nclass Config:\n    DATA_DIR = '/kaggle/input/tusimple-preprocessed/tusimple_preprocessed'\n    \n    IMAGES_DIR = os.path.join(DATA_DIR, 'training/frames')\n    MASKS_DIR = os.path.join(DATA_DIR, 'training/lane-masks')\n    \n    TEST_IMAGES_DIR = os.path.join(DATA_DIR, 'test/frames')\n    TEST_MASKS_DIR = os.path.join(DATA_DIR, 'test/lane-masks')\n    \n    TRAIN_RATIO = 0.8\n    VAL_RATIO = 0.2\n    \n    ENCODER = 'resnet50'\n    ENCODER_WEIGHTS = 'imagenet'\n    CLASSES = 1\n    ACTIVATION = 'sigmoid'\n    \n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    EPOCHS = 50\n    BATCH_SIZE = 16\n    \n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    DROPOUT = 0.1\n    \n    NUM_WORKERS = 4\n    \n    MODEL_DIR = '/kaggle/working/models'\n    BEST_MODEL_PATH = os.path.join(MODEL_DIR, 'best_unet_fixed.pth')\n    LAST_MODEL_PATH = os.path.join(MODEL_DIR, 'last_unet_fixed.pth')\n\ndef create_model(config):\n    model = smp.Unet(\n        encoder_name=config.ENCODER,\n        encoder_weights=config.ENCODER_WEIGHTS,\n        classes=config.CLASSES,\n        activation=config.ACTIVATION,\n        decoder_dropout=config.DROPOUT,\n    )\n    return model\n\ndef create_dataloaders_with_split(config):\n    print(\"📂 Caricamento dataset completo (training + test)...\")\n    \n    full_dataset = LaneSegmentationDataset(\n        images_dir=config.IMAGES_DIR,\n        masks_dir=config.MASKS_DIR,\n        test_images_dir=config.TEST_IMAGES_DIR,\n        test_masks_dir=config.TEST_MASKS_DIR,\n        transform=None,\n        preprocessing=None,\n    )\n    \n    total_size = len(full_dataset)\n    print(f\"✅ Dataset caricato: {total_size} immagini totali\")\n    \n    train_size = int(config.TRAIN_RATIO * total_size)\n    val_size = total_size - train_size\n    \n    print(f\"\\n📊 Split ratio: {config.TRAIN_RATIO*100:.1f}% training / {config.VAL_RATIO*100:.1f}% validation\")\n    print(f\"   Train samples: {train_size}\")\n    print(f\"   Val samples: {val_size}\")\n    \n    train_dataset, val_dataset = random_split(\n        full_dataset,\n        [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    train_dataset_augmented = TrainAugmentedDataset(\n        dataset=train_dataset,\n        transform=get_training_augmentation(),\n    )\n    \n    val_dataset_augmented = ValAugmentedDataset(\n        dataset=val_dataset,\n        transform=get_validation_augmentation(),\n    )\n    \n    train_loader = DataLoader(\n        train_dataset_augmented,\n        batch_size=config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=True,\n        prefetch_factor=2,\n    )\n    \n    val_loader = DataLoader(\n        val_dataset_augmented,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=True,\n        prefetch_factor=2,\n    )\n    \n    print(f\"✅ DataLoaders creati!\")\n    return train_loader, val_loader\n\n\nclass TrainAugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        image, mask = self.dataset[idx]\n        if self.transform:\n            sample = self.transform(image=image, mask=mask)\n            image = sample['image']\n            mask = sample['mask']\n        return image, mask\n\n\nclass ValAugmentedDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        image, mask = self.dataset[idx]\n        if self.transform:\n            sample = self.transform(image=image, mask=mask)\n            image = sample['image']\n            mask = sample['mask']\n        return image, mask\n\n\ndef train_epoch(model, loader, optimizer, loss_fn, device):\n    model.train()\n    total_loss = 0\n    \n    loop = tqdm(loader, desc='Training')\n    for images, masks in loop:\n        images = images.to(device).float()\n        masks = masks.unsqueeze(1).to(device).float()\n        \n        predictions = model(images)\n        loss = loss_fn(predictions, masks)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    return total_loss / len(loader)\n\n\ndef validate_epoch(model, loader, loss_fn, device):\n    \"\"\"\n    ✅ VERSIONE CORRETTA - Metriche calcolate per OGNI IMMAGINE\n    \"\"\"\n    model.eval()\n    \n    total_loss = 0\n    all_metrics = {\n        'iou': [],\n        'dice': [],\n        'sensitivity': [],\n        'specificity': [],\n        'f1': [],\n        'mcc': [],\n        'accuracy': []\n    }\n    \n    with torch.no_grad():\n        loop = tqdm(loader, desc='Validation')\n        for images, masks in loop:\n            images = images.to(device).float()\n            masks = masks.unsqueeze(1).to(device).float()\n            \n            predictions = model(images)\n            loss = loss_fn(predictions, masks)\n            \n            # ✅ CRITICO: Conversione a probabilità e binarizzazione\n            pred_prob = torch.sigmoid(predictions)\n            pred_binary = (pred_prob > 0.5).float()\n            \n            total_loss += loss.item()\n            \n            # ✅ CORRETTO: Per OGNI immagine nel batch\n            for batch_idx in range(pred_prob.shape[0]):\n                # Estrai singola immagine e maschera dal batch\n                pred_single = pred_prob[batch_idx].squeeze()      # [H, W]\n                pred_bin_single = pred_binary[batch_idx].squeeze() # [H, W]\n                mask_single = masks[batch_idx].squeeze()           # [H, W]\n                \n                # ✅ Calcola IoU CORRETTAMENTE\n                intersection = (pred_bin_single * mask_single).sum()\n                union = pred_bin_single.sum() + mask_single.sum() - intersection\n                iou = (intersection + 1e-6) / (union + 1e-6)\n                all_metrics['iou'].append(iou.item())\n                \n                # ✅ Calcola altre metriche\n                all_metrics['dice'].append(LaneMetrics.dice_coefficient(pred_single, mask_single))\n                all_metrics['sensitivity'].append(LaneMetrics.sensitivity(pred_single, mask_single))\n                all_metrics['specificity'].append(LaneMetrics.specificity(pred_single, mask_single))\n                all_metrics['f1'].append(LaneMetrics.f1_score(pred_single, mask_single))\n                all_metrics['mcc'].append(LaneMetrics.mcc(pred_single, mask_single))\n                all_metrics['accuracy'].append(LaneMetrics.pixel_accuracy(pred_single, mask_single))\n            \n            # ✅ Mostra metriche batch corrente\n            batch_sens = sum(all_metrics['sensitivity'][-len(masks):]) / len(masks)\n            batch_spec = sum(all_metrics['specificity'][-len(masks):]) / len(masks)\n            loop.set_postfix(\n                loss=loss.item(),\n                sens=f\"{batch_sens:.3f}\",\n                spec=f\"{batch_spec:.3f}\"\n            )\n    \n    # ✅ Media finale di TUTTE le metriche\n    avg_loss = total_loss / len(loader)\n    \n    metrics_final = {}\n    for k, v in all_metrics.items():\n        metrics_final[k] = sum(v) / len(v) if v else 0.0\n    \n    return avg_loss, metrics_final\n\n\nclass BinaryCrossEntropyWithPosWeight(nn.Module):\n    \"\"\"✅ BCE Loss con class weight dinamico\"\"\"\n    \n    def __init__(self, pos_weight=10.0):\n        super().__init__()\n        self.pos_weight = pos_weight\n    \n    def forward(self, predictions, targets):\n        loss = nn.functional.binary_cross_entropy_with_logits(\n            predictions,\n            targets,\n            pos_weight=torch.tensor(self.pos_weight)\n        )\n        return loss\n\n\nclass WeightedTverskyLoss(nn.Module):\n    \"\"\"✅ Tversky Loss AGGRESSIVA per class imbalance\"\"\"\n    \n    def __init__(self, alpha=0.1, beta=0.9, smooth=1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.smooth = smooth\n    \n    def forward(self, predictions, targets):\n        probs = torch.sigmoid(predictions)\n        \n        TP = (probs * targets).sum()\n        FP = (probs * (1 - targets)).sum()\n        FN = ((1 - probs) * targets).sum()\n        \n        tversky_index = (TP + self.smooth) / (\n            TP + self.alpha * FP + self.beta * FN + self.smooth\n        )\n        \n        return 1 - tversky_index\n\n\nclass CombinedLossAggressivo(nn.Module):\n    \"\"\"✅ Combinazione aggressiva per class imbalance\"\"\"\n    \n    def __init__(self, bce_weight=0.3, tversky_weight=0.7):\n        super().__init__()\n        self.bce_weight = bce_weight\n        self.tversky_weight = tversky_weight\n        \n        self.bce_loss = BinaryCrossEntropyWithPosWeight(pos_weight=10.0)\n        self.tversky_loss = WeightedTverskyLoss(alpha=0.1, beta=0.9)\n    \n    def forward(self, predictions, targets):\n        bce = self.bce_loss(predictions, targets)\n        tversky = self.tversky_loss(predictions, targets)\n        \n        return self.bce_weight * bce + self.tversky_weight * tversky\n\n\ndef main():\n    config = Config()\n    \n    if not os.path.exists(config.IMAGES_DIR):\n        raise FileNotFoundError(f\"❌ Cartella non trovata: {config.IMAGES_DIR}\")\n    if not os.path.exists(config.MASKS_DIR):\n        raise FileNotFoundError(f\"❌ Cartella non trovata: {config.MASKS_DIR}\")\n    \n    os.makedirs(config.MODEL_DIR, exist_ok=True)\n    \n    print(\"=\" * 70)\n    print(\"🚀 TRAINING U-NET - METRICHE CORRETTE\")\n    print(\"🎯 Loss: BCE (pos_weight=10) + Weighted Tversky (alpha=0.1)\")\n    print(\"=\" * 70)\n    print(f\"Encoder: {config.ENCODER}\")\n    print(f\"Device: {config.DEVICE}\")\n    print(f\"Learning rate: {config.LEARNING_RATE}\")\n    print(f\"Epochs: {config.EPOCHS}\")\n    print(\"=\" * 70)\n    \n    model = create_model(config)\n    model.to(config.DEVICE)\n    \n    train_loader, val_loader = create_dataloaders_with_split(config)\n    \n    loss_fn = CombinedLossAggressivo(bce_weight=0.3, tversky_weight=0.7)\n    \n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=config.LEARNING_RATE,\n        weight_decay=config.WEIGHT_DECAY\n    )\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=5,\n        T_mult=2,\n        eta_min=1e-6\n    )\n    \n    best_specificity = 0.0\n    best_metrics = {}\n    no_improve_count = 0\n    patience = 15\n    \n    print(\"\\n🏋️ Inizio training...\\n\")\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\n{'='*70}\")\n        print(f\"Epoch {epoch + 1}/{config.EPOCHS}\")\n        print(f\"{'='*70}\")\n        \n        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, config.DEVICE)\n        val_loss, metrics = validate_epoch(model, val_loader, loss_fn, config.DEVICE)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        scheduler.step()\n        \n        print(f\"\\n📈 RISULTATI VALIDAZIONE:\")\n        print(f\"   Train Loss:     {train_loss:.4f}\")\n        print(f\"   Val Loss:       {val_loss:.4f}\")\n        print(f\"   ───────────────────────\")\n        print(f\"   Sensitivity:    {metrics['sensitivity']:.4f} (corsie trovate)\")\n        print(f\"   🎯 Specificity: {metrics['specificity']:.4f} ← METRICA PRINCIPALE\")\n        print(f\"   Dice:           {metrics['dice']:.4f}\")\n        print(f\"   F1:             {metrics['f1']:.4f}\")\n        print(f\"   IoU:            {metrics['iou']:.4f}\")\n        print(f\"   LR:             {current_lr:.6f}\")\n        \n        torch.save(model.state_dict(), config.LAST_MODEL_PATH)\n        \n        if metrics['specificity'] > best_specificity:\n            best_specificity = metrics['specificity']\n            best_metrics = metrics\n            no_improve_count = 0\n            torch.save(model.state_dict(), config.BEST_MODEL_PATH)\n            print(f\"\\n   ✅ NUOVO MIGLIOR MODELLO!\")\n            print(f\"      Specificity: {best_specificity:.4f}\")\n            print(f\"      Sensitivity: {best_metrics['sensitivity']:.4f}\")\n            print(f\"      F1: {best_metrics['f1']:.4f}\")\n        else:\n            no_improve_count += 1\n        \n        if no_improve_count >= patience:\n            print(f\"\\n⚠️ Early stopping: nessun miglioramento per {patience} epoch\")\n            break\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"✅ TRAINING COMPLETATO!\")\n    print(\"=\" * 70)\n    print(f\"Specificity:  {best_specificity:.4f} (pochi FP) 🎯\")\n    print(f\"Sensitivity:  {best_metrics['sensitivity']:.4f} (corsie trovate)\")\n    print(f\"F1:           {best_metrics['f1']:.4f}\")\n    print(f\"Dice:         {best_metrics['dice']:.4f}\")\n    print(f\"IoU:          {best_metrics['iou']:.4f}\")\n    print(f\"MCC:          {best_metrics['mcc']:.4f}\")\n    print(f\"Modello:      {config.BEST_MODEL_PATH}\")\n    print(\"=\" * 70)\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:08:23.427739Z","iopub.execute_input":"2025-10-25T13:08:23.428141Z","iopub.status.idle":"2025-10-25T13:08:23.442685Z","shell.execute_reply.started":"2025-10-25T13:08:23.428095Z","shell.execute_reply":"2025-10-25T13:08:23.441869Z"}},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile dataset.py\n# dataset_COMBINED.py - COMBINA TRAINING + TEST SET\n\nimport os\nimport cv2\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass LaneSegmentationDataset(Dataset):\n    \"\"\"\n    Dataset personalizzato che carica:\n    - ✅ Training set (frames + lane-masks)\n    - ✅ Test set (frames + lane-masks)\n    \n    Combina entrambi per avere più dati per il training.\n    Il split 80/20 interno creerà nuovi train/val set.\n    \"\"\"\n    \n    def __init__(self, images_dir, masks_dir, test_images_dir=None, test_masks_dir=None, transform=None, preprocessing=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.preprocessing = preprocessing\n        \n        # ✅ Carica TRAINING SET\n        self.images_fps = sorted([\n            os.path.join(images_dir, image_id)\n            for image_id in os.listdir(images_dir)\n            if image_id.endswith(('.jpg', '.png', '.jpeg'))\n        ])\n        \n        self.masks_fps = sorted([\n            os.path.join(masks_dir, image_id)\n            for image_id in os.listdir(masks_dir)\n            if image_id.endswith(('.jpg', '.png', '.jpeg'))\n        ])\n        \n        # ✅ Carica TEST SET (se fornito)\n        if test_images_dir and test_masks_dir:\n            print(f\"\\n📂 Caricamento TEST SET da: {test_images_dir}\")\n            \n            test_images = sorted([\n                os.path.join(test_images_dir, image_id)\n                for image_id in os.listdir(test_images_dir)\n                if image_id.endswith(('.jpg', '.png', '.jpeg'))\n            ])\n            \n            test_masks = sorted([\n                os.path.join(test_masks_dir, image_id)\n                for image_id in os.listdir(test_masks_dir)\n                if image_id.endswith(('.jpg', '.png', '.jpeg'))\n            ])\n            \n            print(f\"✅ Test set immagini trovate: {len(test_images)}\")\n            print(f\"✅ Test set maschere trovate: {len(test_masks)}\")\n            \n            # Combina training + test set\n            self.images_fps.extend(test_images)\n            self.masks_fps.extend(test_masks)\n            \n            print(f\"\\n✅ TOTALE IMMAGINI (Training + Test): {len(self.images_fps)}\")\n        else:\n            print(f\"⚠️ Test set NON fornito - usa solo training set\")\n        \n        # Verifica numero immagini == numero maschere\n        assert len(self.images_fps) == len(self.masks_fps), \\\n            f\"Numero immagini ({len(self.images_fps)}) != numero maschere ({len(self.masks_fps)})\"\n    \n    def __len__(self):\n        return len(self.images_fps)\n    \n    def __getitem__(self, idx):\n        # Leggi immagine RGB\n        image = cv2.imread(self.images_fps[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Leggi mask (grayscale)\n        mask = cv2.imread(self.masks_fps[idx], cv2.IMREAD_GRAYSCALE)\n        \n        # ✅ Binarizza ma NON dividere per 255!\n        mask = (mask > 127).astype(np.float32)\n        \n        # Applica augmentation (se presente)\n        if self.transform:\n            sample = self.transform(image=image, mask=mask)\n            image = sample['image']\n            mask = sample['mask']\n        \n        # Applica preprocessing dell'encoder (se presente)\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image = sample['image']\n            mask = sample['mask']\n        \n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:08:23.443874Z","iopub.execute_input":"2025-10-25T13:08:23.444124Z","iopub.status.idle":"2025-10-25T13:08:23.464123Z","shell.execute_reply.started":"2025-10-25T13:08:23.444087Z","shell.execute_reply":"2025-10-25T13:08:23.463525Z"}},"outputs":[{"name":"stdout","text":"Writing dataset.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile augmentation.py\n# augmentation.py - VERSIONE CON NORMALIZZAZIONE ImageNet\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\ndef get_training_augmentation():\n    \"\"\"\n    Data augmentation per training set.\n    ✅ INCLUDE: Normalizzazione ImageNet + ToTensorV2\n    \"\"\"\n    train_transform = [\n        A.Resize(256, 256),\n        \n        A.HorizontalFlip(p=0.5),\n        A.Affine(\n            scale=(0.9, 1.1),\n            translate_percent=(0.0625, 0.0625),\n            rotate=(-10, 10),\n            p=0.5\n        ),\n        A.OneOf([\n            A.RandomBrightnessContrast(\n                brightness_limit=0.2,\n                contrast_limit=0.2,\n                p=1.0\n            ),\n            A.HueSaturationValue(\n                hue_shift_limit=10,\n                sat_shift_limit=20,\n                val_shift_limit=20,\n                p=1.0\n            ),\n        ], p=0.5),\n        A.GaussianBlur(blur_limit=3, p=0.2),\n        \n        # ✅ CRITICO: Normalizzazione ImageNet\n        # Mean e Std di ImageNet per RGB\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n        ),\n        \n        # ✅ Conversione a PyTorch format (CxHxW)\n        ToTensorV2(),\n    ]\n    \n    return A.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"\n    Augmentation per validation set.\n    ✅ INCLUDE: Normalizzazione ImageNet + ToTensorV2\n    \"\"\"\n    val_transform = [\n        A.Resize(256, 256),\n        \n        # ✅ CRITICO: Normalizzazione ImageNet\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n        ),\n        \n        # ✅ Conversione a PyTorch format (CxHxW)\n        ToTensorV2(),\n    ]\n    return A.Compose(val_transform)\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"\n    Preprocessing specifico dell'encoder.\n    ⚠️ NON usare questo - lo facciamo direttamente con Normalize\n    \"\"\"\n    _transform = [\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ]\n    return A.Compose(_transform)\n\n\ndef get_heavy_augmentation():\n    \"\"\"\n    Augmentation aggressiva per dataset piccoli.\n    ✅ INCLUDE: Normalizzazione ImageNet + ToTensorV2\n    \"\"\"\n    heavy_transform = [\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.2),\n        A.Affine(\n            scale=(0.8, 1.2),\n            translate_percent=(0.1, 0.1),\n            rotate=(-15, 15),\n            shear=(-5, 5),\n            p=0.7\n        ),\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=30, p=1.0),\n            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n        ], p=0.7),\n        A.OneOf([\n            A.GaussianBlur(blur_limit=5, p=1.0),\n            A.MotionBlur(blur_limit=5, p=1.0),\n        ], p=0.3),\n        \n        # ✅ CRITICO: Normalizzazione ImageNet\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n        ),\n        \n        # ✅ Conversione a PyTorch format (CxHxW)\n        ToTensorV2(),\n    ]\n    \n    return A.Compose(heavy_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:23:28.281317Z","iopub.execute_input":"2025-10-25T13:23:28.281597Z","iopub.status.idle":"2025-10-25T13:23:28.288987Z","shell.execute_reply.started":"2025-10-25T13:23:28.281572Z","shell.execute_reply":"2025-10-25T13:23:28.288250Z"}},"outputs":[{"name":"stdout","text":"Overwriting augmentation.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile metrics.py\n# metrics_CORRECTED.py - METRICHE CORRETTE PER LANE DETECTION\n\nimport torch\nimport numpy as np\n\nclass LaneMetrics:\n    \"\"\"Metriche specializzate per lane detection - VERSIONE CORRETTA\"\"\"\n    \n    # ==================== 1️⃣ DICE COEFFICIENT ====================\n    @staticmethod\n    def dice_coefficient(pred, target, smooth=1.0):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: Dice = (2 * TP) / (2 * TP + FP + FN)\n        \"\"\"\n        # Converti a binario (pred potrebbe già essere binario o probabilità)\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        intersection = (pred_binary * target).sum()\n        dice = (2.0 * intersection + smooth) / (pred_binary.sum() + target.sum() + smooth)\n        return dice.item()\n    \n    \n    # ==================== 2️⃣ SENSITIVITY (RECALL) ====================\n    @staticmethod\n    def sensitivity(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: Sensitivity = TP / (TP + FN)\n        \n        ⚠️ BUG ORIGINALE: Non convertiva pred a binario!\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        TP = (pred_binary * target).sum()\n        FN = ((1 - pred_binary) * target).sum()\n        \n        sensitivity = TP / (TP + FN + smooth)\n        return sensitivity.item()\n    \n    \n    # ==================== 3️⃣ SPECIFICITY ====================\n    @staticmethod\n    def specificity(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: Specificity = TN / (TN + FP)\n        \n        ⚠️ BUG ORIGINALE: Non convertiva pred a binario!\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        TN = ((1 - pred_binary) * (1 - target)).sum()\n        FP = (pred_binary * (1 - target)).sum()\n        \n        specificity = TN / (TN + FP + smooth)\n        return specificity.item()\n    \n    \n    # ==================== 4️⃣ F1 SCORE ====================\n    @staticmethod\n    def f1_score(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: F1 = 2 * (Precision * Recall) / (Precision + Recall)\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        TP = (pred_binary * target).sum()\n        FP = (pred_binary * (1 - target)).sum()\n        FN = ((1 - pred_binary) * target).sum()\n        \n        precision = TP / (TP + FP + smooth)\n        recall = TP / (TP + FN + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        \n        return f1.item()\n    \n    \n    # ==================== 5️⃣ MATTHEWS CORRELATION COEFFICIENT ====================\n    @staticmethod\n    def mcc(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n        \n        ⚠️ BUG ORIGINALE: Non gestiva correttamente i tensori\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        TP = (pred_binary * target).sum()\n        TN = ((1 - pred_binary) * (1 - target)).sum()\n        FP = (pred_binary * (1 - target)).sum()\n        FN = ((1 - pred_binary) * target).sum()\n        \n        numerator = TP * TN - FP * FN\n        denominator = torch.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + smooth)\n        \n        # ✅ FIX: Gestisci il caso di denominatore zero\n        if denominator == 0:\n            return 0.0\n        \n        mcc = numerator / denominator\n        return mcc.item()\n    \n    \n    # ==================== 6️⃣ PIXEL ACCURACY ====================\n    @staticmethod\n    def pixel_accuracy(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ CORRETTO\n        Formula: Accuracy = (TP + TN) / Total\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        correct = (pred_binary == target).float().sum()\n        total = target.numel()\n        \n        # ✅ FIX: Gestisci il caso di total zero\n        if total == 0:\n            return 0.0\n        \n        return (correct / total).item()\n    \n    \n    # ==================== 7️⃣ IoU (Intersection over Union) ====================\n    @staticmethod\n    def iou(pred, target, smooth=1e-6):\n        \"\"\"\n        ✅ AGGIUNTO - Lo standard per segmentazione\n        Formula: IoU = TP / (TP + FP + FN)\n        \"\"\"\n        # ✅ FIX: Converti a binario\n        if pred.max() > 1.0:\n            pred_binary = (pred > 0.5).float()\n        else:\n            pred_binary = pred\n        \n        target = target.float()\n        \n        intersection = (pred_binary * target).sum()\n        union = pred_binary.sum() + target.sum() - intersection\n        \n        iou_score = (intersection + smooth) / (union + smooth)\n        return iou_score.item()\n\n\n# ==================== FUNZIONE DI UTILITÀ ====================\n\ndef calculate_all_metrics(pred_batch, target_batch):\n    \"\"\"\n    ✅ CORRETTA - Calcola tutte le metriche per un batch\n    \n    Input:\n        pred_batch: tensor [B, H, W] con probabilità [0, 1] o valori > 1\n        target_batch: tensor [B, H, W] con valori binari {0, 1}\n    \n    Output:\n        dict con tutte le metriche\n    \"\"\"\n    metrics = {\n        'iou': [],\n        'dice': [],\n        'sensitivity': [],\n        'specificity': [],\n        'f1': [],\n        'mcc': [],\n        'accuracy': [],\n    }\n    \n    # ✅ FIX: Itera correttamente su ogni elemento del batch\n    for batch_idx in range(pred_batch.shape[0]):\n        pred_single = pred_batch[batch_idx]    # [H, W]\n        target_single = target_batch[batch_idx] # [H, W]\n        \n        # Calcola tutte le metriche per questa immagine\n        metrics['iou'].append(LaneMetrics.iou(pred_single, target_single))\n        metrics['dice'].append(LaneMetrics.dice_coefficient(pred_single, target_single))\n        metrics['sensitivity'].append(LaneMetrics.sensitivity(pred_single, target_single))\n        metrics['specificity'].append(LaneMetrics.specificity(pred_single, target_single))\n        metrics['f1'].append(LaneMetrics.f1_score(pred_single, target_single))\n        metrics['mcc'].append(LaneMetrics.mcc(pred_single, target_single))\n        metrics['accuracy'].append(LaneMetrics.pixel_accuracy(pred_single, target_single))\n    \n    # ✅ Restituisci media di tutte le metriche\n    return {k: np.mean(v) if v else 0.0 for k, v in metrics.items()}\n\n\n# ==================== DEBUG ====================\n\nif __name__ == '__main__':\n    print(\"✅ Test metriche corrette...\\n\")\n    \n    # Crea dati fittizi\n    pred = torch.rand(256, 256)           # Probabilità [0, 1]\n    target = torch.randint(0, 2, (256, 256)).float()  # Binario\n    \n    print(\"📊 Metriche per Lane Detection:\\n\")\n    print(f\"  IoU:              {LaneMetrics.iou(pred, target):.4f}\")\n    print(f\"  Dice:             {LaneMetrics.dice_coefficient(pred, target):.4f}\")\n    print(f\"  Sensitivity:      {LaneMetrics.sensitivity(pred, target):.4f} ← Corsie trovate\")\n    print(f\"  Specificity:      {LaneMetrics.specificity(pred, target):.4f} ← Falsi positivi\")\n    print(f\"  F1 Score:         {LaneMetrics.f1_score(pred, target):.4f}\")\n    print(f\"  MCC:              {LaneMetrics.mcc(pred, target):.4f}\")\n    print(f\"  Accuracy:         {LaneMetrics.pixel_accuracy(pred, target):.4f}\")\n    \n    # Test batch\n    print(\"\\n\\n📊 Test Batch:\\n\")\n    pred_batch = torch.rand(4, 256, 256)\n    target_batch = torch.randint(0, 2, (4, 256, 256)).float()\n    \n    all_metrics = calculate_all_metrics(pred_batch, target_batch)\n    for k, v in all_metrics.items():\n        print(f\"  {k.upper():12s}: {v:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:08:23.483822Z","iopub.execute_input":"2025-10-25T13:08:23.484024Z","iopub.status.idle":"2025-10-25T13:08:23.502239Z","shell.execute_reply.started":"2025-10-25T13:08:23.484001Z","shell.execute_reply":"2025-10-25T13:08:23.501634Z"}},"outputs":[{"name":"stdout","text":"Writing metrics.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:08:23.503615Z","iopub.execute_input":"2025-10-25T13:08:23.503865Z","iopub.status.idle":"2025-10-25T13:09:41.425357Z","shell.execute_reply.started":"2025-10-25T13:08:23.503849Z","shell.execute_reply":"2025-10-25T13:09:41.424586Z"}},"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.0.0rc2)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (11.3.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.5.3)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.0.19)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.3)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.28.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.19.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub>=0.24->segmentation_models_pytorch) (8.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (1.3.1)\nDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation_models_pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation_models_pytorch-0.5.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:23:34.105706Z","iopub.execute_input":"2025-10-25T13:23:34.105971Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n======================================================================\n🚀 TRAINING U-NET - METRICHE CORRETTE\n🎯 Loss: BCE (pos_weight=10) + Weighted Tversky (alpha=0.1)\n======================================================================\nEncoder: resnet50\nDevice: cuda\nLearning rate: 0.001\nEpochs: 50\n======================================================================\n📂 Caricamento dataset completo (training + test)...\n\n📂 Caricamento TEST SET da: /kaggle/input/tusimple-preprocessed/tusimple_preprocessed/test/frames\n✅ Test set immagini trovate: 2782\n✅ Test set maschere trovate: 2782\n\n✅ TOTALE IMMAGINI (Training + Test): 6408\n✅ Dataset caricato: 6408 immagini totali\n\n📊 Split ratio: 80.0% training / 20.0% validation\n   Train samples: 5126\n   Val samples: 1282\n✅ DataLoaders creati!\n\n🏋️ Inizio training...\n\n\n======================================================================\nEpoch 1/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:59<00:00,  5.42it/s, loss=0.712]\nValidation: 100%|█| 81/81 [00:09<00:00,  8.28it/s, loss=0.715, sens=0.726, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7309\n   Val Loss:       0.7178\n   ───────────────────────\n   Sensitivity:    0.7204 (corsie trovate)\n   🎯 Specificity: 0.4838 ← METRICA PRINCIPALE\n   Dice:           0.1071\n   F1:             0.1071\n   IoU:            0.0423\n   LR:             0.001000\n\n   ✅ NUOVO MIGLIOR MODELLO!\n      Specificity: 0.4838\n      Sensitivity: 0.7204\n      F1: 0.1071\n\n======================================================================\nEpoch 2/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:58<00:00,  5.49it/s, loss=0.729]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.25it/s, loss=0.711, sens=0.726, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7185\n   Val Loss:       0.7143\n   ───────────────────────\n   Sensitivity:    0.7181 (corsie trovate)\n   🎯 Specificity: 0.4889 ← METRICA PRINCIPALE\n   Dice:           0.1076\n   F1:             0.1076\n   IoU:            0.0423\n   LR:             0.000905\n\n   ✅ NUOVO MIGLIOR MODELLO!\n      Specificity: 0.4889\n      Sensitivity: 0.7181\n      F1: 0.1076\n\n======================================================================\nEpoch 3/50\n======================================================================\nTraining: 100%|████████████████████| 321/321 [00:58<00:00,  5.49it/s, loss=0.71]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.19it/s, loss=0.712, sens=0.723, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7166\n   Val Loss:       0.7118\n   ───────────────────────\n   Sensitivity:    0.7233 (corsie trovate)\n   🎯 Specificity: 0.4884 ← METRICA PRINCIPALE\n   Dice:           0.1084\n   F1:             0.1083\n   IoU:            0.0423\n   LR:             0.000655\n\n======================================================================\nEpoch 4/50\n======================================================================\nTraining: 100%|████████████████████| 321/321 [00:58<00:00,  5.50it/s, loss=0.72]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.13it/s, loss=0.711, sens=0.722, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7147\n   Val Loss:       0.7101\n   ───────────────────────\n   Sensitivity:    0.7235 (corsie trovate)\n   🎯 Specificity: 0.4900 ← METRICA PRINCIPALE\n   Dice:           0.1087\n   F1:             0.1087\n   IoU:            0.0423\n   LR:             0.000346\n\n   ✅ NUOVO MIGLIOR MODELLO!\n      Specificity: 0.4900\n      Sensitivity: 0.7235\n      F1: 0.1087\n\n======================================================================\nEpoch 5/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:58<00:00,  5.50it/s, loss=0.711]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.32it/s, loss=0.711, sens=0.722, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7133\n   Val Loss:       0.7092\n   ───────────────────────\n   Sensitivity:    0.7241 (corsie trovate)\n   🎯 Specificity: 0.4907 ← METRICA PRINCIPALE\n   Dice:           0.1089\n   F1:             0.1089\n   IoU:            0.0423\n   LR:             0.000096\n\n   ✅ NUOVO MIGLIOR MODELLO!\n      Specificity: 0.4907\n      Sensitivity: 0.7241\n      F1: 0.1089\n\n======================================================================\nEpoch 6/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:58<00:00,  5.50it/s, loss=0.715]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.51it/s, loss=0.71, sens=0.726, spec=0\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7175\n   Val Loss:       0.7127\n   ───────────────────────\n   Sensitivity:    0.7190 (corsie trovate)\n   🎯 Specificity: 0.4899 ← METRICA PRINCIPALE\n   Dice:           0.1080\n   F1:             0.1080\n   IoU:            0.0423\n   LR:             0.001000\n\n======================================================================\nEpoch 7/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:58<00:00,  5.50it/s, loss=0.706]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.28it/s, loss=0.709, sens=0.727, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7164\n   Val Loss:       0.7117\n   ───────────────────────\n   Sensitivity:    0.7232 (corsie trovate)\n   🎯 Specificity: 0.4886 ← METRICA PRINCIPALE\n   Dice:           0.1084\n   F1:             0.1084\n   IoU:            0.0423\n   LR:             0.000976\n\n======================================================================\nEpoch 8/50\n======================================================================\nTraining: 100%|████████████████████| 321/321 [00:58<00:00,  5.51it/s, loss=0.71]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.40it/s, loss=0.71, sens=0.725, spec=0\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7160\n   Val Loss:       0.7110\n   ───────────────────────\n   Sensitivity:    0.7230 (corsie trovate)\n   🎯 Specificity: 0.4895 ← METRICA PRINCIPALE\n   Dice:           0.1085\n   F1:             0.1085\n   IoU:            0.0423\n   LR:             0.000905\n\n======================================================================\nEpoch 9/50\n======================================================================\nTraining: 100%|███████████████████| 321/321 [00:58<00:00,  5.50it/s, loss=0.719]\nValidation: 100%|█| 81/81 [00:08<00:00,  9.46it/s, loss=0.708, sens=0.729, spec=\n\n📈 RISULTATI VALIDAZIONE:\n   Train Loss:     0.7149\n   Val Loss:       0.7105\n   ───────────────────────\n   Sensitivity:    0.7231 (corsie trovate)\n   🎯 Specificity: 0.4899 ← METRICA PRINCIPALE\n   Dice:           0.1086\n   F1:             0.1086\n   IoU:            0.0423\n   LR:             0.000794\n\n======================================================================\nEpoch 10/50\n======================================================================\nTraining:  49%|█████████▎         | 157/321 [00:28<00:29,  5.55it/s, loss=0.712]","output_type":"stream"}],"execution_count":null}]}